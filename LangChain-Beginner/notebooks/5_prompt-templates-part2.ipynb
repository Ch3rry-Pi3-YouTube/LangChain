{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa881af",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>LangChain: Prompt Templates (Part 2)</strong>\n",
    "</h1>\n",
    "\n",
    "Welcome back! In this notebook, we compare **PromptTemplate** and **ChatPromptTemplate** side-by-side.\n",
    "\n",
    "The key idea: PromptTemplate builds **one string**, while ChatPromptTemplate builds a **structured list of messages**. That structure is why ChatPromptTemplate is the preferred approach for chat, history, and tools.\n",
    "\n",
    "**Why this matters:**\n",
    "- Chat models expect role-based messages under the hood\n",
    "- History stays clean and typed (no fake USER/ASSISTANT labels)\n",
    "- It scales to tools/agents without changing the pattern\n",
    "\n",
    "We will show the same task both ways so you can see where string prompts become brittle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4412c",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Setup: model and environment</strong>\n",
    "</h2>\n",
    "\n",
    "We will reuse a lightweight model and load `.env` for the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9abfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dotenv loader\n",
    "from dotenv import load_dotenv\n",
    "# Import the chat model initializer\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Load variables from .env into the process environment\n",
    "load_dotenv()\n",
    "\n",
    "# Create a lightweight chat model for this demo\n",
    "model = init_chat_model(\n",
    "    # Choose a small, fast OpenAI model\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c076a",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Approach A: PromptTemplate (history as text)</strong>\n",
    "</h2>\n",
    "\n",
    "We will do the string version first. With PromptTemplate, history is just a **string blob**. You must invent labels like USER/ASSISTANT and keep the formatting consistent yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4506e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the string prompt template class\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a plain string prompt that includes conversation history\n",
    "string_prompt = PromptTemplate.from_template(\n",
    "    # The history and question are injected as text\n",
    "    \"You are a concise tutor.\\n\\n\"\n",
    "    \"Conversation so far:\\n\"\n",
    "    \"{history}\\n\\n\"\n",
    "    \"Next user question:\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    "    \"Answer in one sentence\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0649a4",
   "metadata": {},
   "source": [
    "We build the string history and the next question.\n",
    "\n",
    "Notice how the roles are just text labels, not real message objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e1b8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a manual text history (note the fake USER/ASSISTANT labels)\n",
    "string_history = \"USER: What is overfitting?\\nASSISTANT: Overfitting is when a model learns noise and performs poorly on new data.\"\n",
    "\n",
    "# Define the next user question\n",
    "question = \"Give me one quick way to reduce it.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04bbee2",
   "metadata": {},
   "source": [
    "We fill the template to create the final prompt string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad13bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the string template with history and the next question\n",
    "final_string_prompt = string_prompt.invoke(\n",
    "    {\n",
    "        \"history\": string_history,\n",
    "        \"question\": question\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ccdd66",
   "metadata": {},
   "source": [
    "Now we send the string prompt to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b212e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: One quick way to reduce overfitting is to use regularization techniques, such as L1 or L2 regularization.\n"
     ]
    }
   ],
   "source": [
    "# Send the final string prompt to the model\n",
    "response = model.invoke(final_string_prompt)\n",
    "\n",
    "# Print the assistant response\n",
    "print(\"Assistant:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316a314",
   "metadata": {},
   "source": [
    "**Output note:** One sentence back - nice. But notice we're still in string-land."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b61ae0",
   "metadata": {},
   "source": [
    "To continue the chat, we manually append the new turn back into the history string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63bb4355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is overfitting?\n",
      "ASSISTANT: Overfitting is when a model learns noise and performs poorly on new data.\n",
      "USER: Give me one quick way to reduce it.\n",
      "ASSISTANT: One quick way to reduce overfitting is to use regularization techniques, such as L1 or L2 regularization.\n"
     ]
    }
   ],
   "source": [
    "# Manually append the new turn back into the history text\n",
    "string_history += f\"\\nUSER: {question}\\nASSISTANT: {response.content}\"\n",
    "\n",
    "# Print the updated history string\n",
    "print(string_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038ff4e",
   "metadata": {},
   "source": [
    "**Output note:** The history grew, but it's just a text blob you have to maintain by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14cc8b5",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Approach B: ChatPromptTemplate (history as messages)</strong>\n",
    "</h2>\n",
    "\n",
    "Now we switch to the message-based version. Here, history stays **typed** as message objects, which is safer and scales to tools and multi-turn chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8861780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import chat prompt tools and message classes\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "from langchain.messages import HumanMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940adf6",
   "metadata": {},
   "source": [
    "We define a chat prompt with a `MessagesPlaceholder` for history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e6a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat prompt with a history placeholder\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # System rule for the assistant\n",
    "        (\"system\", \"You are a concise tutor. Answer in one sentence.\"),\n",
    "        # Insert past messages here\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        # Next user question\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f977e",
   "metadata": {},
   "source": [
    "We create real `HumanMessage` and `AIMessage` objects for history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad444f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create structured history as real messages\n",
    "chat_history = [\n",
    "    HumanMessage(\n",
    "        content=\"What is overfitting?\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"Overfitting is when a model learns noise and performs poorly on new data.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Reuse the same question\n",
    "question = \"Give me one quick way to reduce it.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576b3fb",
   "metadata": {},
   "source": [
    "Finally, we format the template into a message list that is ready to send to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6131c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the chat template with history and the next question\n",
    "chat_prompt_value = chat_prompt.invoke(\n",
    "    {\n",
    "        \"history\": chat_history,\n",
    "        \"question\": question\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert the prompt value into messages ready for the model\n",
    "messages_to_send = chat_prompt_value.to_messages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf4486",
   "metadata": {},
   "source": [
    "Now we send the **message list** to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "897a0e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: One quick way to reduce overfitting is to use regularization techniques, such as L1 or L2 regularization.\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(messages_to_send)\n",
    "\n",
    "print(\"Assistant:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd7f89",
   "metadata": {},
   "source": [
    "**Output note:** One sentence again, but now it comes from structured messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b594e",
   "metadata": {},
   "source": [
    "Here we append the new user + assistant messages back into history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc5e932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is overfitting?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Overfitting is when a model learns noise and performs poorly on new data.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Give me one quick way to reduce it.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='One quick way to reduce overfitting is to use regularization techniques, such as L1 or L2 regularization.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Give me one quick way to reduce it.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='One quick way to reduce overfitting is to use regularization techniques, such as L1 or L2 regularization.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_history.append(HumanMessage(content=question))\n",
    "chat_history.append(AIMessage(content=response.content))\n",
    "\n",
    "display(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb41d39",
   "metadata": {},
   "source": [
    "**Output note:** You should see a list of HumanMessage and AIMessage objects with the new turn appended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7dc150",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Key takeaways</strong>\n",
    "</h2>\n",
    "\n",
    "- `PromptTemplate` -> outputs **one string**; you must manage history formatting yourself.\n",
    "- `ChatPromptTemplate` -> outputs **messages**; history stays structured and reusable.\n",
    "- For real chat apps (memory, tools, multi-turn), **ChatPromptTemplate is the safer default**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-beginner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}