{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796b63df",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>LangChain: Multi-Turn Chat</strong>\n",
    "</h1>\n",
    "\n",
    "In this notebook, we build a short conversation by **manually tracking history**. Each new message is added to a list, and that list is sent back to the model every turn.\n",
    "\n",
    "This pattern is the foundation for chat memory, multi-step workflows, and more advanced LangChain apps.\n",
    "\n",
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Loading environmental variables</strong>\n",
    "</h2>\n",
    "\n",
    "Quick setup: load the .env file so our API key is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cc2f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dotenv loader\n",
    "from dotenv import load_dotenv\n",
    "# Import the chat model initializer\n",
    "from langchain.chat_models import init_chat_model\n",
    "# Import explicit message types for a conversation\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Load variables from .env into the process environment\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812780b9",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Initialising the chat model and history</strong>\n",
    "</h2>\n",
    "\n",
    "We initialise a lightweight model and start an empty history list. This list will hold **System**, **Human**, and **AI** messages in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c89182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lightweight chat model for this demo\n",
    "model = init_chat_model(\n",
    "    # Choose a small, fast OpenAI model\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# Start an empty conversation history\n",
    "history = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e15af",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Setting the system instruction</strong>\n",
    "</h2>\n",
    "\n",
    "We begin with a `SystemMessage` that controls the assistant's behavior.\n",
    "\n",
    "Output note: the printout shows a Python list containing a single `SystemMessage` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3a2c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First memory:\n",
      "\n",
      "[SystemMessage(content='You are a concise tutor. Keep answers within 30 words.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Add a system instruction to guide the assistant\n",
    "history.append(\n",
    "    SystemMessage(\n",
    "        # Keep responses short and focused\n",
    "        content=\"You are a concise tutor. Keep answers within 30 words.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the history after the first message\n",
    "print(f\"First memory:\\n\\n{history}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbe468",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Turn 1: Ask the first question</strong>\n",
    "</h2>\n",
    "\n",
    "We add a `HumanMessage`, send the full history to the model, and then store the reply as an `AIMessage`.\n",
    "\n",
    "Output note: the assistant prints a short definition of overfitting (kept concise by the system rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0d7ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Overfitting occurs when a model learns training data too well, capturing noise instead of patterns, leading to poor performance on new, unseen data.\n"
     ]
    }
   ],
   "source": [
    "# Add the user's first question\n",
    "history.append(\n",
    "    HumanMessage(\n",
    "        # First user query\n",
    "        content=\"What is overfitting?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Send the full history to the model\n",
    "ai_1 = model.invoke(history)\n",
    "\n",
    "# Store the assistant response in history\n",
    "history.append(\n",
    "    AIMessage(\n",
    "        # Capture just the model's text\n",
    "        content=ai_1.content\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the assistant's reply\n",
    "print(\"Assistant:\", ai_1.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed61c2",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Turn 2: Follow-up with context</strong>\n",
    "</h2>\n",
    "\n",
    "We add another `HumanMessage` that depends on the first answer, and send the **entire history** again.\n",
    "\n",
    "Output note: the assistant responds with a practical way to reduce overfitting (e.g., regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e86f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Using regularization techniques, such as L1 or L2 regularization, can help reduce overfitting by penalizing overly complex models.\n"
     ]
    }
   ],
   "source": [
    "# Add a follow-up question that depends on context\n",
    "history.append(\n",
    "    HumanMessage(\n",
    "        # Second user query\n",
    "        content=\"Give one practical way to reduce it.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Send the updated history to the model\n",
    "ai_2 = model.invoke(history)\n",
    "\n",
    "# Store the assistant response in history\n",
    "history.append(\n",
    "    AIMessage(\n",
    "        # Capture just the model's text\n",
    "        content=ai_2.content\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the assistant's reply\n",
    "print(\"Assistant:\", ai_2.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3dc775",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Reviewing the full conversation</strong>\n",
    "</h2>\n",
    "\n",
    "Finally, we pretty-print every message in order.\n",
    "\n",
    "Output note: you should see a role-labeled transcript with the system instruction, both user questions, and both assistant replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "926fd224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a concise tutor. Keep answers within 30 words.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is overfitting?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Overfitting occurs when a model learns training data too well, capturing noise instead of patterns, leading to poor performance on new, unseen data.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Give one practical way to reduce it.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Using regularization techniques, such as L1 or L2 regularization, can help reduce overfitting by penalizing overly complex models.\n"
     ]
    }
   ],
   "source": [
    "# Pretty-print every message in the conversation\n",
    "for h in history:\n",
    "    # Show each message in a readable format\n",
    "    h.pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-beginner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}