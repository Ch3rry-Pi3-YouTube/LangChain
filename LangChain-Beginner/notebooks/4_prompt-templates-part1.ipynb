{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed676ba8",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>LangChain: Prompt Templates (Part 1)</strong>\n",
    "</h1>\n",
    "\n",
    "Prompt templates let you write a prompt once, then fill in the blanks. This makes prompts easy to reuse and easy to debug.\n",
    "\n",
    "In LangChain v1, templates live in `langchain_core.prompts`.\n",
    "\n",
    "We will cover three practical patterns:\n",
    "- `PromptTemplate` for plain strings\n",
    "- `ChatPromptTemplate` for system + human messages\n",
    "- using templates directly with a chat model\n",
    "\n",
    "Quick tip: it is always worth printing the final prompt before you call a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdac1d",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Example 1: PromptTemplate (string prompts)</strong>\n",
    "</h2>\n",
    "\n",
    "We will start small with a plain string template. It outputs **one string**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the string prompt template class\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reusable string prompt template\n",
    "string_prompt = PromptTemplate.from_template(\n",
    "    # Placeholders live inside {braces}\n",
    "    \"Explain {topic} to a {audience} in exactly 2 sentences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9953c7",
   "metadata": {},
   "source": [
    "Now we **fill the template** and print the final string.\n",
    "\n",
    "If the line matches, your template is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24a7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain overfitting to a beginner in exactly 2 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Fill in the placeholders (LangChain calls this invoke)\n",
    "string_prompt_value = string_prompt.invoke(\n",
    "    # Provide values for each placeholder\n",
    "    {\n",
    "        \"topic\": \"overfitting\",\n",
    "        \"audience\": \"beginner\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the final formatted prompt\n",
    "print(string_prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84598e40",
   "metadata": {},
   "source": [
    "**Output note:** You should see the filled-in prompt line. If placeholders did not fill, double-check the dictionary keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351695aa",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Example 2: ChatPromptTemplate (system + human)</strong>\n",
    "</h2>\n",
    "\n",
    "Now we switch to a chat prompt. This one outputs **multiple messages**, each with a role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f8e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the chat prompt template class\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a83ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat prompt from role + message pairs\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # System message sets the behavior\n",
    "        (\"system\", \"You are a helpful tutor. Answer in {format_style}.\"),\n",
    "        # Human message is the user request\n",
    "        (\"human\", \"Teach me {topic} using one simple example\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52cd7fd",
   "metadata": {},
   "source": [
    "We define a system rule and a human request, then fill the placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df46ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful tutor. Answer in two bullet points.\n",
      "HUMAN: Teach me data leakage using one simple example\n"
     ]
    }
   ],
   "source": [
    "# Fill in placeholders across all chat messages\n",
    "chat_prompt_value = chat_prompt.invoke(\n",
    "    # Provide values for format and topic\n",
    "    {\n",
    "        \"format_style\": \"two bullet points\",\n",
    "        \"topic\": \"data leakage\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the final messages with their roles\n",
    "for message in chat_prompt_value.to_messages():\n",
    "    # Each message has a type and content\n",
    "    print(f\"{message.type.upper()}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6ce81",
   "metadata": {},
   "source": [
    "**Output note:** You will see SYSTEM: and HUMAN: lines - that's the exact message list the model will receive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92bcb60",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Initialising the model</strong>\n",
    "</h2>\n",
    "\n",
    "We will reuse a lightweight chat model. (The `.env` load is just for your API key.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5098d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dotenv and the chat model initializer\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Load variables from .env into the process environment\n",
    "load_dotenv()\n",
    "\n",
    "# Create a lightweight chat model for this demo\n",
    "model = init_chat_model(\n",
    "    # Choose a small, fast OpenAI model\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4212bed8",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Example 3: Template + model</strong>\n",
    "</h2>\n",
    "\n",
    "First, we send the **string prompt** to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749432b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting occurs when a machine learning model learns the training data too well, capturing noise and outliers rather than the underlying patterns. As a result, the model performs poorly on new, unseen data because it has become too tailored to the specifics of the training set.\n"
     ]
    }
   ],
   "source": [
    "# Send the string prompt to the model\n",
    "response = model.invoke(string_prompt_value.to_string())\n",
    "\n",
    "# Print the assistant response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311dc67",
   "metadata": {},
   "source": [
    "**Output note:** Expect two sentences. The wording can vary - that's normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358a724",
   "metadata": {},
   "source": [
    "Now we send the **chat prompt messages** to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c0eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Example Scenario**: Consider a model predicting house prices based on various features like square footage, number of bedrooms, and location. If the dataset includes the final sale price of the houses as a feature used to train the model, this would cause data leakage because the model has access to information from the future (the actual sale price) that it would not have in a real-world scenario.\n",
      "\n",
      "- **Impact of Data Leakage**: This leads to overly optimistic model performance during training and validation, as the model learns to predict the sale price based on information it shouldn't have access to, ultimately resulting in poor performance on unseen data.\n"
     ]
    }
   ],
   "source": [
    "# Send the chat prompt messages to the model\n",
    "response = model.invoke(chat_prompt_value.to_messages())\n",
    "\n",
    "# Print the assistant response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927f425",
   "metadata": {},
   "source": [
    "**Output note:** Expect two bullet points, because the system message asked for them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-beginner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}