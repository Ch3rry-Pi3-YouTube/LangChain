{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b77cdc5",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>LangChain: Simple Chat Model</strong>\n",
    "</h1>\n",
    "\n",
    "Welcome! In this notebook, we’ll build the **smallest possible LangChain chat example** and then peek under the hood to see what the model *actually* returns.\n",
    "\n",
    "We’ll:\n",
    "- load environment variables\n",
    "- initialise a lightweight chat model\n",
    "- send a quick prompt\n",
    "- inspect the `AIMessage` object\n",
    "- convert it into a plain Python dictionary\n",
    "- and grab the text in the cleanest way possible\n",
    "\n",
    "By the end, you’ll know exactly what a LangChain chat response looks like — and how to work with it like regular Python data.\n",
    "\n",
    "\n",
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Loading environmental variables</strong>\n",
    "</h2>\n",
    "\n",
    "Before we do anything, it's a good idea to load our environment variables.\n",
    "\n",
    "The `python-dotenv` library reads this file and adds the variables to the process environment, making them accessible to our model via `os.getenv(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6641b9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24833acf",
   "metadata": {},
   "source": [
    "Next, we want to create a **chat model** instance using LangChain.\n",
    "\n",
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Initialising the chat model</strong>\n",
    "</h2>\n",
    "\n",
    "The `init_chat_model` helper function selects and configures the appropriate chat model based on the arguments provided. In this case, we are using a lightweight OpenAI model suitable for simple examples and demonstrations.\n",
    "\n",
    "The returned `model` object acts as a callable interface to the underlying language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "becac87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ddfa9a",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Sending a prompt to the model</strong>\n",
    "</h2>\n",
    "\n",
    "With the chat model initialised, we can now send it an input prompt (e.g., **\"Hiya!\"**).\n",
    "\n",
    "The `invoke()` method executes a single model call and returns the model’s response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa4538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(input=\"Hiya!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c205dcf",
   "metadata": {},
   "source": [
    "Now, let's check out the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78724e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 10, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-D1HbVuqzrvbHqIp0TQTIx8GcbQnwZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bec70-b845-7483-bd84-00a843dcce74-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 10, 'output_tokens': 10, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb082494",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Inspecting the model response</strong>\n",
    "</h2>\n",
    "\n",
    "When we call a chat model in LangChain, the result is **not just a plain string**.\n",
    "\n",
    "Instead, LangChain returns an **`AIMessage` object**. This object contains:\n",
    "\n",
    "- the assistant’s text reply (`content`)\n",
    "- metadata about the call (model name, token usage, finish reason, etc.)\n",
    "- tool-call fields (empty in this simple example)\n",
    "\n",
    "At first glance, this can look confusing because it’s a rich Python object, not a “chat bubble”.\n",
    "\n",
    "To confirm what it actually is, let’s check its Python type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e9fb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd421d72",
   "metadata": {},
   "source": [
    "Think of `langchain_core.messages.ai.AIMessage` as a fully-qualified “address” for a Python class inside the LangChain codebase.\n",
    "\n",
    "It can be broken down like this:\n",
    "\n",
    "- `langchain_core` → a Python **package**\n",
    "\n",
    "- `messages` → a **sub-package** / module namespace inside langchain_core related to chat messages\n",
    "\n",
    "- `ai` → a **module** (a .py file) inside messages that defines AI-related message types\n",
    "\n",
    "- `AIMessage` → a **class** defined in that module\n",
    "\n",
    "So, `AIMessage` is a class, located at:\n",
    "\n",
    "<p align=\"center\">\n",
    "  package <code>langchain_core</code> → subpackage <code>messages</code> → module <code>ai</code> → class <code>AIMessage</code>\n",
    "</p>\n",
    "\n",
    "**BUT...!** \n",
    "\n",
    "Didn't we only install the `langchain` package, and not `langchain_core`?\n",
    "\n",
    "Yes, but when we install `langchain`, we *implicitly* install `langchain-core`. We can prove this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0922d63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.7\n"
     ]
    }
   ],
   "source": [
    "import langchain_core\n",
    "print(langchain_core.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a06f8",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Converting the response into a plain Python dictionary</strong>\n",
    "</h2>\n",
    "\n",
    "Now, let’s take the `AIMessage` object returned by the model and convert it into a **standard Python dictionary**.\n",
    "\n",
    "LangChain message objects are built using **Pydantic**, which means they provide a `.model_dump()` method. This method extracts all fields from the object into a plain data structure that is easier to inspect, log, or serialise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54886921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_kwargs': {'refusal': None},\n",
      " 'content': 'Hi there! How can I assist you today?',\n",
      " 'id': 'lc_run--019bec70-b845-7483-bd84-00a843dcce74-0',\n",
      " 'invalid_tool_calls': [],\n",
      " 'name': None,\n",
      " 'response_metadata': {'finish_reason': 'stop',\n",
      "                       'id': 'chatcmpl-D1HbVuqzrvbHqIp0TQTIx8GcbQnwZ',\n",
      "                       'logprobs': None,\n",
      "                       'model_name': 'gpt-4o-mini-2024-07-18',\n",
      "                       'model_provider': 'openai',\n",
      "                       'service_tier': 'default',\n",
      "                       'system_fingerprint': 'fp_29330a9688',\n",
      "                       'token_usage': {'completion_tokens': 10,\n",
      "                                       'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                                                     'audio_tokens': 0,\n",
      "                                                                     'reasoning_tokens': 0,\n",
      "                                                                     'rejected_prediction_tokens': 0},\n",
      "                                       'prompt_tokens': 10,\n",
      "                                       'prompt_tokens_details': {'audio_tokens': 0,\n",
      "                                                                 'cached_tokens': 0},\n",
      "                                       'total_tokens': 20}},\n",
      " 'tool_calls': [],\n",
      " 'type': 'ai',\n",
      " 'usage_metadata': {'input_token_details': {'audio': 0, 'cache_read': 0},\n",
      "                    'input_tokens': 10,\n",
      "                    'output_token_details': {'audio': 0, 'reasoning': 0},\n",
      "                    'output_tokens': 10,\n",
      "                    'total_tokens': 20}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "response_dict = response.model_dump()\n",
    "pprint(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2cba54",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Understanding the dictionary output</strong>\n",
    "</h2>\n",
    "\n",
    "This is the same model response as before, but now represented as a **plain Python dictionary** rather than a LangChain object.\n",
    "\n",
    "Key fields to notice:\n",
    "\n",
    "- **`content`**  \n",
    "  The actual text generated by the model.\n",
    "\n",
    "- **`type`**  \n",
    "  Indicates the role of the message. Here it is `\"ai\"`, meaning the response came from the model.\n",
    "\n",
    "- **`response_metadata`**  \n",
    "  Information about how the response was generated, including:\n",
    "  - the model used\n",
    "  - why the generation stopped (`finish_reason`)\n",
    "  - token usage for the prompt and completion\n",
    "\n",
    "- **`usage_metadata`**  \n",
    "  A simplified summary of input and output tokens, useful for tracking cost and performance.\n",
    "\n",
    "- **`tool_calls` / `invalid_tool_calls`**  \n",
    "  Empty in this example because no tools were used.\n",
    "\n",
    "At this stage, there is nothing “LangChain-specific” about the data structure — it is just ordinary Python data that can be logged, stored, or converted to JSON.\n",
    "\n",
    "For example, we can extract the content using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f2fc3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! How can I assist you today?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_dict[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59c4b3",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Pretty-printing the response</strong>\n",
    "</h2>\n",
    "\n",
    "Because `response` is an `AIMessage` object (not a plain string), printing it directly can look noisy and hard to read.\n",
    "\n",
    "LangChain provides a built-in helper method, `.pretty_print()`, which formats the message in a clean, human-readable way. This is especially useful in notebooks and live demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1368330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df830f3",
   "metadata": {},
   "source": [
    "This output shows the assistant’s message clearly, without any of the surrounding metadata.\n",
    "\n",
    "Behind the scenes, the `AIMessage` object still contains additional information such as token usage and model details — `.pretty_print()` simply presents the most relevant part for humans: the generated text.\n",
    "\n",
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Accessing the message content directly</strong>\n",
    "</h2>\n",
    "\n",
    "If all you care about is the text generated by the model, you can access it directly using the `content` attribute.\n",
    "\n",
    "This is often the most convenient option when building simple applications or when you want behaviour that feels like a traditional chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9460005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937e0aa",
   "metadata": {},
   "source": [
    "Here, the response is returned as a plain Python string.\n",
    "\n",
    "At this point, all of the additional metadata has been ignored, and we are working only with the assistant’s text output. This is typically what you would pass into downstream application logic or display to an end user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-beginner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
