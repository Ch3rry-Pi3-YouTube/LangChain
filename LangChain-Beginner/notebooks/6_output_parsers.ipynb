{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>LangChain: Output Parsers</strong>\n",
    "</h1>\n",
    "\n",
    "In this notebook we learn **Output Parsers** in LangChain v1.\n",
    "\n",
    "You will see three parsers:\n",
    "- **StrOutputParser** -> plain Python string\n",
    "- **JsonOutputParser** -> Python dict\n",
    "- **PydanticOutputParser** -> validated, typed objects\n",
    "\n",
    "**Why this matters:** LLMs generate text. Output parsers turn that text into **reliable data** your code can safely use.\n",
    "\n",
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Mental model (what is flowing through the chain)</strong>\n",
    "</h2>\n",
    "\n",
    "LangChain v1 treats prompts, models, and parsers as **Runnables**. Each step transforms one object into another:\n",
    "\n",
    "```\n",
    "input variables\n",
    "   -> PromptTemplate / ChatPromptTemplate\n",
    "   -> PromptValue (messages)\n",
    "   -> ChatModel\n",
    "   -> AIMessage\n",
    "   -> OutputParser\n",
    "   -> Python object (str / dict / BaseModel)\n",
    "```\n",
    "\n",
    "Each parser answers one question:\n",
    "**\"Given the model output, how do I turn this into something my code can safely use?\"**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Setup: model and environment</strong>\n",
    "</h2>\n",
    "\n",
    "We load the API key and initialize a small chat model for consistent examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fc5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Pull values from .env into the process env\n",
    "load_dotenv()\n",
    "\n",
    "# Import the chat model initializer\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Guardrail: ensure the API key is present\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999ef040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a lightweight chat model\n",
    "model = init_chat_model(\n",
    "    # Small, fast model for demos\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca5ac06",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Example 1: StrOutputParser (plain strings)</strong>\n",
    "</h2>\n",
    "\n",
    "**When to use:** When you want the model response as a **plain Python string**, not an AIMessage object.\n",
    "\n",
    "**Intuition (under the hood):**\n",
    "- The prompt formats variables into **message objects** (System/Human).\n",
    "- The model returns an **AIMessage** (it is not a string yet).\n",
    "- `StrOutputParser` extracts **only** the `.content` and returns a `str`.\n",
    "\n",
    "Think of it as the bridge from **LLM world** to **normal Python**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28bee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the chat prompt template + string parser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Build a simple prompt with roles\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # System message sets the behavior\n",
    "        (\"system\", \"You are a concise tutor. Reply in one sentence.\"),\n",
    "        # Human message is the actual question\n",
    "        (\"human\", \"Explain {topic}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the simplest parser (AIMessage -> str)\n",
    "parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17129a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose the pipeline: prompt -> model -> parser\n",
    "chain = prompt | model | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b31df6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Data leakage refers to the unintentional exposure of sensitive data to unauthorized individuals, often occurring during the data handling or processing stages in machine learning or analytics.\n"
     ]
    }
   ],
   "source": [
    "# Run the chain with a topic variable\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"topic\": \"data leakage\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Confirm the type and value\n",
    "print(type(response))\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210320f7",
   "metadata": {},
   "source": [
    "**Output note:** The result is a **Python string**, not an AIMessage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52f871",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Example 2: JsonOutputParser (JSON -> dict)</strong>\n",
    "</h2>\n",
    "\n",
    "**When to use:** When you want a dictionary back (so you can do `result[\"answer\"]`).\n",
    "\n",
    "**Intuition (under the hood):**\n",
    "- The model still returns **text**.\n",
    "- The parser attempts `json.loads(...)` on that text.\n",
    "- If JSON is valid, you get a **dict**. If not, it fails loudly.\n",
    "\n",
    "This is the key shift from **best-effort text** to **explicit success or failure**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JSON parser\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "\n",
    "# Build a prompt that demands JSON only\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Return ONLY valid JSON. No markdown, no extra text.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Answer the question and also provide a confidence interval score.\\n\"\n",
    "            \"Question: {question}\\n\\n\"\n",
    "            \"Return JSON with keys: answer (string), confidence (number 0-1).\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# JSON parser attempts json.loads on the model output\n",
    "parser = JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feea9588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'answer': \"Overfitting is a modeling error that occurs when a machine learning model learns the noise and fluctuations in the training data to the extent that it negatively impacts the model's performance on new data. This means that while the model may perform exceptionally well on the training dataset, it fails to generalize to unseen data, leading to poor predictive performance.\", 'confidence': 0.95}\n",
      "dict_keys(['answer', 'confidence'])\n",
      "\n",
      "answer = Overfitting is a modeling error that occurs when a machine learning model learns the noise and fluctuations in the training data to the extent that it negatively impacts the model's performance on new data. This means that while the model may perform exceptionally well on the training dataset, it fails to generalize to unseen data, leading to poor predictive performance.\n",
      "\n",
      "confidence = 0.95\n"
     ]
    }
   ],
   "source": [
    "# Compose the pipeline and run it\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What is overfitting?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Inspect the parsed dict\n",
    "print(type(response))\n",
    "print(response)\n",
    "print(response.keys())\n",
    "print(\"\\nanswer =\", response[\"answer\"])\n",
    "print(\"\\nconfidence =\", response[\"confidence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd27235",
   "metadata": {},
   "source": [
    "**Output note:** You should see a `dict` with keys like `answer` and `confidence`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe357c",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Example 3: PydanticOutputParser (typed objects)</strong>\n",
    "</h2>\n",
    "\n",
    "**When to use:** When you want **validation** and **typed outputs**.\n",
    "\n",
    "**Intuition (under the hood):**\n",
    "- You define a **schema** with Pydantic.\n",
    "- The parser generates **format instructions** for the model.\n",
    "- The model returns JSON text.\n",
    "- LangChain parses JSON and then **validates** it with Pydantic.\n",
    "- If validation fails, the chain errors early. If it passes, you get a real object.\n",
    "\n",
    "This turns LLM output into **validated domain objects**, which is much safer for real apps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c100f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pydantic and the output parser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Build a prompt that will include parser instructions\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a careful tutor. Follow the formatting instructions exactly.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Define the term.\\n\"\n",
    "            \"Term: {term}\\n\\n\"\n",
    "            \"Formatting instructions:\\n{format_instructions}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the schema we want back\n",
    "class Definition(BaseModel):\n",
    "    term: str = Field(description=\"The term being defined.\")\n",
    "    definition: str = Field(description=\"A clear definition in plain English.\")\n",
    "    example: str = Field(description=\"A short example.\")\n",
    "\n",
    "# Create the parser bound to the schema\n",
    "parser = PydanticOutputParser(pydantic_object=Definition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ca1540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Definition'>\n",
      "term='regularisation' definition='Regularisation is a technique used in machine learning and statistics to prevent overfitting by adding a penalty term to the loss function, which helps to simplify the model.' example='In linear regression, L2 regularisation (also known as Ridge regression) adds a penalty equal to the square of the magnitude of the coefficients to the loss function.'\n",
      "\n",
      "term = regularisation\n",
      "\n",
      "definition = Regularisation is a technique used in machine learning and statistics to prevent overfitting by adding a penalty term to the loss function, which helps to simplify the model.\n",
      "\n",
      "example = In linear regression, L2 regularisation (also known as Ridge regression) adds a penalty equal to the square of the magnitude of the coefficients to the loss function.\n"
     ]
    }
   ],
   "source": [
    "# Compose and run the chain\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"term\": \"regularisation\",\n",
    "        # Use the parser-provided format instructions\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Inspect the typed result\n",
    "print(type(response))\n",
    "print(response)\n",
    "print(\"\\nterm =\", response.term)\n",
    "print(\"\\ndefinition =\", response.definition)\n",
    "print(\"\\nexample =\", response.example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output note:** The result is a `Definition` instance with `.term`, `.definition`, `.example`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\" style=\"color: #1f77b4;\">\n",
    "  <strong>Key takeaways</strong>\n",
    "</h2>\n",
    "\n",
    "- **StrOutputParser**: best for plain strings.\n",
    "- **JsonOutputParser**: best for simple structured data as dicts.\n",
    "- **PydanticOutputParser**: best for validated, typed outputs.\n",
    "\n",
    "One clean teaching line:\n",
    "> Output parsers turn an LLM from a text generator into a dependable software component.\n",
    "\n",
    "If you want the next lesson to flow from here, a great follow-up is:\n",
    "**\"What happens when parsing fails, and how do we recover?\"**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-beginner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
